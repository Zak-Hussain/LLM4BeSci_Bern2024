{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8df0bbd",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "In this notebook, we will explore the capabilities of transformer-based models for natural language processing (NLP) using the Hugging Face `transformers` library. We will use the `sentence-transformers` package to extract features from text data and the `transformers` library to perform text generation tasks.\n",
    "\n",
    "By the end of this exercise, you will have learned how to:\n",
    "- Extract features from text data using transformer-based models\n",
    "- Generate text using transformer-based models\n",
    "- Access the largest and latest open models through the Hugging Face API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979416b3c30fb86b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using Notebook Environments \n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c616368742ccde73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install --upgrade transformers sentence-transformers pacmap accelerate &> /dev/null\n",
    "\n",
    "    # Change working directory to day_1\n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_Bern2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c24c2ed829f80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames. We here make use three packages:\n",
    "\n",
    "1. `pandas`: A very popular package for reading and manipulating data in python.\n",
    "2. `sentence_transformers`: A package for extracting features from text data using transformer-based models.\n",
    "3. `transformers`: A HF package for loading and manipulating transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fad62d6f82855",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606abea72b904fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature Extraction: Relating Personality Measures\n",
    "\n",
    "In this analysis--inspired by  [Wulff & Mata, 2023](https://osf.io/preprints/psyarxiv/9h7aw)--we will use an LLM to extract features from personality items. In this case, features are *numerical* representations of the meaning (and syntax) of text. We will extract features using the `sentence-transformers` package.\n",
    "The code will use the extracted features to compute the similarity between personality items. It then evaluates how well these similarities predict the observed similarities between items. Finally, it visualises the item similarity space in two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pacmap import PaCMAP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8ab6925055a21da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code begins by loading the personality items into a `pandas.DataFrame` with three columns:\n",
    "\n",
    "1. `factor`: The (high-level) personality factor to which the item belongs.\n",
    "2. `construct`: The (mid-level) personality construct to which the item belongs.\n",
    "3. `item`: The text of the personality item used to measure the construct.\n",
    "\n",
    "Run the cell below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6bf097acf12ea11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loading personality data\n",
    "personality = pd.read_csv('items.csv') \n",
    "personality"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a69c9fa229eb6df3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting Features from Personality Items\n",
    "The code makes use of the `all-MiniLM-L6-v2` model, which is a small and efficient embedding model, to extract features from the sentences. The model will encode the sentences into 384-dimensional vector representations. The cell will then print the features as a pandas dataframe for easy viewing. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb6f4ec05f3a3993"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca015d4695f403",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "item_features = model.encode(personality['item'])\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "item_features = pd.DataFrame(item_features, index=personality['item'])\n",
    "item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing Similarity between Personality Items\n",
    "Now that we have extracted features for each personality item, we can compute the similarity between items. We use the `sklearn`'s `cosine similarity` function, which measures the cosine of the angle between two vectors. The closer the cosine similarity is to 1, the more similar the two items are. We compute the similarity between all pairs of items and store the results in a similarity matrix.\n",
    "\n",
    "Run the cell below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e432180c4407d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compute cosine similarity between features\n",
    "predicted_sims = cosine_similarity(item_features)\n",
    "predicted_sims = pd.DataFrame(predicted_sims, index=personality['item'], columns=personality['item'])\n",
    "predicted_sims"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17422d544ee8930",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the similarity matrix is symmetric, with the diagonal containing 1s (since the similarity of an item with itself is 1). Furthermore, items that you would expect to be more related (e.g. \"Turn plans into actions\" and \"Plunge into tasks with all my heart.\" are indeed more similar. Conversely, less related items (e.g. \"Am calm in tense situations\" and \"Demand quality\") show lower cosine similarities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35335594ddfaa712"
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Comparing to observed correlations between items\n",
    "This section compares how well the predicted similarities align with the *observed* similarities between items: that is, the correlations between the participant responses to the items. It first loads the observed correlations into a `pandas.DataFrame`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "404b2cc55d085c99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load observed correlations\n",
    "observed_sims = pd.read_csv('item_corrs.csv')\n",
    "observed_sims"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba31561155f02ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the code pivots `observed_sims` to create a correlation matrix with the same structure as `predicted_sims` so that they can be easily compared."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73ac898a3e520635"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Pivoting to a correlation matrix for easy comparison with predicted correlations\n",
    "observed_sims = observed_sims.pivot(index='text_i', columns='text_j', values='cor')\n",
    "observed_sims"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e8452423206924",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The predicted and observed similarities are then aligned to ensure that the items are in the same order. The code then flattens the lower triangle of the matrices into vectors to compute the correlation between the predicted and observed similarities."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10cdf340cd6cf7c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Aligning observed and predicted similarities\n",
    "predicted_sims, observed_sims = predicted_sims.align(observed_sims)\n",
    "\n",
    "def lower_triangle_flat(df):\n",
    "    \"\"\"Takes the lower triangle of a dataframe and flattens it into a vector\"\"\"\n",
    "    rows, cols = np.triu_indices(len(df), k=1)  # k=1 to exclude the diagonal (self-similarities)\n",
    "    return pd.Series(df.values[rows, cols])\n",
    "\n",
    "# Flatten the lower triangle of the observed and predicted similarities into vectors\n",
    "predicted_sims_flat, observed_sims_flat = lower_triangle_flat(predicted_sims), lower_triangle_flat(observed_sims)\n",
    "\n",
    "# Correlation between predicted and observed\n",
    "print(f'r: {predicted_sims_flat.corr(observed_sims_flat).round(2)}')\n",
    "print(f'r of absolute values: {predicted_sims_flat.abs().corr(observed_sims_flat.abs()).round(2)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e18036ed0acfe20",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The correlation between the predicted and observed similarities is 0.18. If we take the absolute values of the similarities, the correlation increases to 0.23. Since we are not interested in which way round (in terms of polarity) the personality item scale was rated, we focus on the absolute values. This suggests that the extracted features capture some of the variance in the observed similarities between items. Whilst this suggests that the extracted features may not be capturing everything we want to know about the items, alternative explanations exist. Can you think of any?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92b1a1b8a5cde1f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing the Item Similarities\n",
    "We can also visualize `predicted_sims` in two dimensions using PaCMAP. PaCMAP is a dimensionality reduction technique that preserves the pairwise distances between points. The code fits the PaCMAP model to the extracted features and transform them into two dimensions, saving the results in a `pandas.DataFrame`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a856558aa34cc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize MDS model\n",
    "pac = PaCMAP(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the features\n",
    "pac_features = pac.fit_transform(item_features)\n",
    "\n",
    "# Convert features to DataFrame\n",
    "pac_features = pd.DataFrame(pac_features, columns=['x', 'y'])\n",
    "pac_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c706fe41d09e9df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, the code adds the personality factors and items as columns to `pac_features` to see how items cluster based on their similarity. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6291ebd6b7320d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adding personality factors to MDS features\n",
    "pac_features['factor'] = personality['factor']\n",
    "pac_features['item'] = personality['item']\n",
    "pac_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81fc2959cfd772f1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code next plots the MDS features, with each point representing a personality item. The points are colored by factor, allowing us to see how items cluster based on their similarity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b4c817604acb060"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot pac features\n",
    "sns.scatterplot(data=pac_features, x='x', y='y', hue='factor', s=100)\n",
    "sns.despine(offset=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "169aae43d068b59d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As illustrated, the items somewhat cluster according to their factor, again suggesting that the extracted features have captured some meaningful information about the items."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc8ad7690f00b50c"
  },
  {
   "cell_type": "markdown",
   "id": "efc8d5804ede6511",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text Generation: Phi-3 Takes the Berlin Numeracy Test\n",
    "In this analysis, we will explore the usage of causal LLMs as synthetic participants in a psychological experiment. We will again use Phi-3, this time to solve the [Berlin Numeracy Test](https://doi.org/10.1017/S1930297500001819). This is a widely used test to measure an individual's ability to understand and apply statistical concepts. "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import textwrap"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfcce48961ac3a08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96eb23af195ac2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1 = \"\"\"\n",
    "Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many times would this five-sided die show an odd number (1, 3 or 5)?\n",
    "\"\"\"\n",
    "\n",
    "q2 = \"\"\"\n",
    "Out of 1,000 people in a small town 500 are members of a choir. Out of these 500 members in the choir 100 are men. Out of the 500 inhabitants that are not in the choir 300 are men. What is the probability that a randomly drawn man is a member of the choir? (please indicate the probability in percent).\n",
    "\"\"\"\n",
    "\n",
    "q3 = \"\"\"\n",
    "Imagine we are throwing a loaded die (6 sides). The probability that the die shows a 6 is twice as high as the probability of each of the other numbers. On average, out of these 70 throws, how many times would the die show the number 6?\n",
    "\"\"\"\n",
    "\n",
    "q4 = \"\"\"\n",
    "In a forest 20% of mushrooms are red, 50% brown and 30% white. A red mushroom is poisonous with a probability of 20%. A mushroom that is not red is poisonous with a probability of 5%. What is the probability that a poisonous mushroom in the forest is red?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a24c50147f2cf7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It next loads Phi-3. Since we will be using Phi-3 to generate text, Phi-3 is loaded using the `transformers` `AutoModelForCausalLM` class. We also load the `AutoTokenizer` class to tokenize the input text."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42) # Set seed for reproducibility\n",
    "\n",
    "# Load Phi-3\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"cuda\", # Use GPU\n",
    "    torch_dtype=torch.float16,  # Use float16 for faster inference\n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager' # Faster inference on T4 GPUs\n",
    ")\n",
    "\n",
    "# Load tokenizer`\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f6f17c2f1ee0f87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code next initialises the familiar `'text-generation'` `pipeline` from day 3. \n",
    "\n",
    "In `generation_args` (which is later fed into the `pipeline` during generation), the code sets `\"max_new_tokens\": 100` to limit the length of the model output and `\"do_sample\": False` to use of *greedy decoding*, which means that the model will always choose the most likely token at each step. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d48b5b2ec896a72d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Define generation arguments\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 100,  # Maximum number of tokens to generate\n",
    "    \"return_full_text\": False, # Return only the generated text\n",
    "    \"do_sample\": False, # Use greedy decoding (incompatible with temperature>0)\n",
    "    # \"temperature\": 0.0  # Uncomment this line to set the temperature parameter\n",
    "}\n",
    "\n",
    "# Loop through questions and generate responses\n",
    "for i, question in enumerate([q1, q2, q3, q4]):\n",
    "    print('-------------------------')   \n",
    "    \n",
    "    # Define prompt\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a thoughtful participant in a psychology experiment. Please answer the following question making your answer as brief as possible (less than 50 words).\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]  # Define prompt with JSON structure\n",
    "    \n",
    "    # Generate response and access generated text at index 0 and key 'generated_text'\n",
    "    response = pipe(prompt, **generation_args)[0]['generated_text']\n",
    "    \n",
    "    # Format question and response for printing\n",
    "    question = '\\n'.join(textwrap.wrap(question, 100))\n",
    "    response = '\\n'.join(textwrap.wrap(response, 100))\n",
    "    print(f\"Question {i+1}: {question} \\n\\nAnswer: {response}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826db144688f9c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the Hugging Face API\n",
    "This section demonstrates how to use the Hugging Face API to do the same as the above numeracy test example. The main benefit of the API is that it allows us to run the latest, largest models without having the specialised hardware needed to run them (since the models are run on the cloud). We will use the `meta-llama/Meta-Llama-3-70B-Instruct` model. \n",
    "\n",
    "In order to use the Hugging Face API, you will need to follow the following steps:\n",
    "\n",
    "1.  Make sure you have a Hugging Face account (https://huggingface.co/join)\n",
    "2. Go to the [Llama-3 model page](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) and fill in the 'META LLAMA 3 COMMUNITY LICENSE AGREEMENT' form at the top of the page in order to get access to the model (this can take up to an hour). \n",
    "3. Once you have been granted access, you can navigate to [in your Hugging Face profile settings](https://huggingface.co/settings/tokens) to get the model's API access token. This token should provide access to all models in the Llama family. \n",
    "4. If you wish to run the largest (70B parameter) version of Llama-3, you will need to have a Hugging Face PRO subscription (currently $9/month). You can also run the smaller versions (as in this notebook) for free.\n",
    "\n",
    "The Hugging Face API code begins importing the `InferenceClient` class from the `huggingface_hub` library. The `InferenceClient` provides a high-level API to interact with models hosted on the Hugging Face Hub. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71ee704ec3d1a77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "138a2b5d251bc167"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "api_key = '<your access token here>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "for i, question in enumerate([q1, q2, q3, q4]):\n",
    "    print('-------------------------')   \n",
    "    \n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a thoughtful participant in a psychology experiment. Please answer the following question making your answer as brief as possible (less than 50 words).\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]  # Define prompt with JSON structure\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=prompt,\n",
    "        model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    ).choices[0].content\n",
    "    \n",
    "    # Format question and response for printing\n",
    "    question = '\\n'.join(textwrap.wrap(question, 100))\n",
    "    response = '\\n'.join(textwrap.wrap(response, 100))\n",
    "    print(f\"Question {i+1}: {question} \\n\\nAnswer: {response}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b9f0256dd2b8ffd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
